{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mne\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.filters.git\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.utils.git\n!pip install bctpy\n!pip install pingouin","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:20:42.034313Z","iopub.execute_input":"2021-09-02T13:20:42.034693Z","iopub.status.idle":"2021-09-02T13:21:45.049452Z","shell.execute_reply.started":"2021-09-02T13:20:42.034614Z","shell.execute_reply":"2021-09-02T13:21:45.048552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gráficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n# Preprocesado y análisis\n# ==============================================================================\nimport statsmodels.api as sm\nimport pingouin as pg\nfrom scipy import stats\nfrom scipy.stats import pearsonr\nfrom scipy.signal import butter, lfilter, welch, hanning, filtfilt, hilbert\n# Configuración matplotlib\n# ==============================================================================\n# plt.style.use('ggplot')\n\n# Configuración warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n#-----------------------------------------------------------------------------------------------------------------\nimport pickle\nimport numpy as np\nfrom scipy.io import loadmat,savemat\nfrom mne.preprocessing import ICA\n# from google.colab import drive\nimport tensorflow_probability as tfp\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport mne\n#-----------------------------------------------------------------------------------------------------------------\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa\nfrom scipy.spatial.distance import squareform \nimport matplotlib.colors as colors\nimport matplotlib.cm as cmx\nfrom gcpds.utils.visualizations import plot_eeg, plot_topoplot\n# from mne.viz.topomap import _check_outlines, _draw_outlines\nfrom mne.viz.utils import plt_show,tight_layout\nfrom mne.io.pick import _picks_to_idx\nfrom mne.viz import plot_topomap\n#-----------------------------------------------------------------------------------------------------------------\nfrom sklearn.metrics import pairwise_distances  \nfrom scipy.spatial.distance import squareform \nfrom scipy.stats import kurtosis\nimport pywt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.signal import periodogram, welch\nfrom scipy.stats import spearmanr\nimport matplotlib.colors as colors\nimport matplotlib.cm as cmx\nfrom mne.time_frequency import tfr_array_morlet, csd_array_morlet, csd_array_fourier\nfrom joblib import Parallel, delayed\nfrom scipy.stats import kendalltau\nimport scipy.io as sio\nimport numpy as np\nfrom joblib import load\nimport bct\n#-----------------------------------------------------------------------------------------------------------------\n# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow_probability as tfp\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import regularizers\nfrom sklearn.base import  BaseEstimator, TransformerMixin, ClassifierMixin\nfrom sklearn.model_selection import train_test_split\nimport types\nimport tempfile\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:21:45.051414Z","iopub.execute_input":"2021-09-02T13:21:45.05182Z","iopub.status.idle":"2021-09-02T13:21:55.171531Z","shell.execute_reply.started":"2021-09-02T13:21:45.051775Z","shell.execute_reply":"2021-09-02T13:21:55.170504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sujetos de la base de datos\nSubjects = list(np.arange(1,22))\n# canales de la base de datos\nchannels = ['Fp1','Fp2','F7','F3','Fz','F4','F8','T3','C3','Cz','C4','T4',\n            'T5','P3','Pz','P4','T6','O1','O2']\nn_channels = len(channels)\n# Frecuencia de muestreo de la base de datos.\nsampling_freq  = 1000\n# Número de trials en resting\nN_tr = 20\n# ubicación de la base de datos.\nload_path = '../input/bcmi-music/Data 1/'\n# Método utilizado para la función de ICA de MNE.\nmethod = 'fastica'\n# Número de núcleos para ell multiproceso\n# pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n# ciclo de los sujetos.\ny = list()\ndata_ref = list()\ndata_ref_ica = list()\nfor s in Subjects:\n  # carga de la información del sujetos.\n  S_ERP = loadmat(load_path + f\"P_\"+str(s)+\"/P\"+str(s)+\"_BCMI_frontHN_2017.mat\")\n  print('sujeto: ', s)\n  data_m = S_ERP['music'][0][0][0]\n  data_m = np.transpose(data_m, (2,1,0)) # transponer en trials x canales x tiempo\n  data_target_m = data_m\n  data_r = S_ERP['base']\n  data_r = np.transpose(data_r, (1,0)) # transponer en canales x tiempo\n  data_r = data_r[:n_channels,10*sampling_freq:-10*sampling_freq]\n  data_target_r = np.zeros((N_tr,data_r.shape[0],9500))\n  step = 9.5*sampling_freq\n  windows = list(np.arange(0,data_r.shape[1],step))\n  for tr in range(N_tr):\n    data_target_r[tr,:,:] = data_r[:,int(windows[tr]):int(windows[tr]+step)]\n  data_target = np.concatenate([data_target_m,data_target_r],axis=0)\n  # data_target = data_target_m\n\n  # structure mne datos en (trials,canales,tiempo)\n  ch_types = ['eeg']*n_channels\n  info = mne.create_info(channels, ch_types=ch_types, sfreq=sampling_freq)\n  info.set_montage('standard_1005')\n  raw = mne.EpochsArray(data_target, info)\n\n  # señal sin quitar los artefactos.\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(data_target_m[0,:,:],channels,sampling_freq)\n  # plt.title('Señal trial 1 de música-sin filtrar')\n\n  # filtro la señal de 0.5 - 100 Hz.\n  raw.filter(0.5,100)\n\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(raw.get_data()[0,:,:],channels,sampling_freq)\n  # plt.title('Señal trial 1 de música-filtrada de 0.5 a 100 Hz')\n\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(data_target_r[0,:,:],channels,sampling_freq)\n  # plt.title('Señal trial 1 de resting-sin filtrar')\n\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(raw.get_data()[13,:,:],channels,sampling_freq)\n  # plt.title('Señal trial 1 de música-filtrada de 0.5 a 100 Hz')\n\n  # Repairing artifacts with ICA\n  # ica.plot_sources(raw, show_scrollbars=False)\n  # ica = ICA(n_components=n_channels, method=method, max_iter='auto', random_state=0)\n  # ica.fit(raw)\n  # componentes = ica.get_sources(raw).get_data()\n  \n  # # señal sin quitar los artefactos.\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(componentes[0,:,:],channels,sampling_freq)\n  \n  # Parametros para el análisis de la entropia.\n  # tau     = 10              # 20   30 \n  # window  = 1*sampling_freq # 3,   3   segundo de la ventana.\n  # step    = window*0.5      # 50%, 80% de traslape.\n  # windows = list(np.arange(0,componentes.shape[2]-window,step))\n\n  # start_time = time.time()\n  # entropy_tr = list()\n  # for tr in range(data_target.shape[0]):\n  #   data = list()\n  #   for time_ in range(len(windows)):\n  #     signal = np.squeeze(raw.get_data()[tr,:,int(windows[time_]):int(windows[time_]+window)])\n  #     data.append(signal)\n  #   # estimación de la entropía de la señal.\n  #   entropy_tr.append(np.asarray(pool.map(fun_entropy, data)))\n  # entropy_raw = np.asarray(entropy_tr)\n  # print('Tiempo de la entropia de componentes {tim}'.format(tim=(time.time()-start_time)))\n  \n  # ica.plot_sources(mne.io.RawArray(raw.get_data()[0], info))\n  \n  # start_time = time.time()\n  # entropy_tr = list()\n  # for tr in range(data_target.shape[0]):\n  #   data = list()\n  #   for time_ in range(len(windows)):\n  #     signal = np.squeeze(componentes[tr,:,int(windows[time_]):int(windows[time_]+window)])\n  #     data.append(signal)\n  #   # estimación de la entropía de las componentes.\n  #   entropy_tr.append(np.asarray(pool.map(fun_entropy, data)))\n  # entropy_com = np.asarray(entropy_tr)\n  # print('Tiempo de la entropia de componentes {tim}'.format(tim=(time.time()-start_time)))\n  \n  # raw2 = raw.copy()\n  # comp_ = []\n  # for tr in range(entropy_com.shape[0]):\n  #   datos_ = np.max(entropy_com[tr,:,:],axis=0)-np.min(entropy_com[0,:,:],axis=0)\n  #   datos_f = np.asarray([round(a,1) for a in datos_])\n  #   comp_.append(np.where((datos_f>0)==True)[0])\n  # pos = list(np.unique(np.concatenate(comp_)))\n  # pos.extend([0,1])\n  # ica.exclude = [0,1]          # indices chosen based on various plots above sujeto 2.\n  # ica.apply(raw2)\n  \n  # figura señal quitando artefactos\n  # plt.figure(figsize=(20,8))\n  # plot_eeg(raw2.get_data()[0,:,:],channels,sampling_freq)\n\n  # start_time = time.time()\n  # entropy_tr = list()\n  # for tr in range(data_target.shape[0]):\n  #   data = list()\n  #   for time_ in range(len(windows)):\n  #     signal = np.squeeze(raw2.get_data()[tr,:,int(windows[time_]):int(windows[time_]+window)])\n  #     data.append(signal)\n  #   # estimación de la entropía de la señal quitando artefactos.\n  #   entropy_tr.append(np.asarray(pool.map(fun_entropy, data)))\n  # entropy_ica = np.asarray(entropy_tr)\n  # print('Tiempo de la entropia de la señal {tim}'.format(tim=(time.time()-start_time)))\n\n  # CAR common average reference\n  # En la señal.\n  rereferenced_raw, ref_data = mne.set_eeg_reference(raw, ref_channels='average',projection=True,verbose=0)\n  # data_ref.append(rereferenced_raw.get_data())\n  data_ref.append(rereferenced_raw.get_data())\n  # En la señal reconstruida.\n  # rereferenced_raw_ica, ref_data = mne.set_eeg_reference(raw2, ref_channels='average',projection=True,verbose=0)\n  # data_ref_ica.append(rereferenced_raw.get_data())\n  # data_ref_ica.append(rereferenced_raw_ica.get_data())\n  # savemat(load_path+f'Resultado2_music_rest_Sujeto'+str(s)+'.mat',{'X_ica':raw2.get_data(),'X_raw':raw.get_data(),'ref_raw':data_ref,\n  #           'ref_ica':data_ref_ica,'H_raw':entropy_raw,'H_com':entropy_com,'H_ica':entropy_ica})","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:21:55.173301Z","iopub.execute_input":"2021-09-02T13:21:55.173604Z","iopub.status.idle":"2021-09-02T13:22:28.257555Z","shell.execute_reply.started":"2021-09-02T13:21:55.173576Z","shell.execute_reply":"2021-09-02T13:22:28.256474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSD por ventanas de tiempo de 1 seg","metadata":{}},{"cell_type":"code","source":"# Calculate PSD\nnblock = 1000\nwin = hanning(nblock, True)\nwindow = 1*sampling_freq\nstep = 0.1*sampling_freq # traslape 90%\nwindows = list(np.arange(0,data_ref[0].shape[2]-window,step))\ndata_power_ = list()\n\nfor sub in range(len(Subjects)):\n  Power = list()\n  for tr in range(data_ref[sub].shape[0]):\n    Power_ven = list()\n    for ven in range(len(windows)):\n      freqs, Pxxf = welch(np.squeeze(data_ref[sub][tr,:,int(windows[ven]):int(windows[ven]+window)]), sampling_freq, window=win,  nfft=nblock, return_onesided=True, scaling='spectrum')\n      Power_ven.append(Pxxf[:,8:13])\n    Power.append(np.asarray(Power_ven))\n  data_power_.append(np.asarray(Power))\nData_pow = np.asarray(data_power_)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:22:28.259067Z","iopub.execute_input":"2021-09-02T13:22:28.259423Z","iopub.status.idle":"2021-09-02T13:22:56.055064Z","shell.execute_reply.started":"2021-09-02T13:22:28.259383Z","shell.execute_reply":"2021-09-02T13:22:56.053985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ACC anteriores","metadata":{}},{"cell_type":"code","source":"resultado_j = loadmat('../input/main-music/Main_music.mat')\nAcc_ = np.mean(resultado_j['facc_'],axis=1)\nAcc_.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:46:56.146621Z","iopub.execute_input":"2021-09-01T05:46:56.147061Z","iopub.status.idle":"2021-09-01T05:46:56.168967Z","shell.execute_reply.started":"2021-09-01T05:46:56.147029Z","shell.execute_reply":"2021-09-01T05:46:56.168127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ACC Nuevos","metadata":{}},{"cell_type":"code","source":"resultado_j = loadmat('../input/mainmusic2/Main_music2.mat')\nAcc_ = np.mean(resultado_j['facc_'],axis=1)\nAcc_.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:23:36.823104Z","iopub.execute_input":"2021-09-02T13:23:36.823558Z","iopub.status.idle":"2021-09-02T13:23:36.8415Z","shell.execute_reply.started":"2021-09-02T13:23:36.823524Z","shell.execute_reply":"2021-09-02T13:23:36.840555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organizar características y acierto ","metadata":{}},{"cell_type":"code","source":"met = 1\n# Acc = np.zeros()\nif met == 1:\n  Acc = Acc_\n  datos = np.reshape(Data_pow,(Data_pow.shape[0],Data_pow.shape[1]*\n                               Data_pow.shape[2]*Data_pow.shape[3]*\n                               Data_pow.shape[4]))\nelif met == 2:\n  # datos = np.zeros((21,912000))\n  Acc = np.zeros((21,1))\n  index = list()\n  for sub in range(len(Acc_)):\n    index.append(resultado_j['facc_'][sub,:]>=Acc_[sub])\n  index_ = np.sum(np.asarray(index),axis=0).reshape(-1,1).T>10\n  Acc = np.mean(resultado_j['facc_'][:,index_.reshape(-1)],axis=1)\n  datos_ = Data_pow[:,:,index_.reshape(-1),:,:]\n  datos = np.reshape(datos_,(datos_.shape[0],datos_.shape[1]*datos_.shape[2]*\n                             datos_.shape[3]*datos_.shape[4]))\nelse:\n  datos = np.zeros((21,912000))\n  Acc = np.zeros((21,1))\n  index = list()\n  tam = list()\n  for sub in range(len(Acc_)):\n    index_ = resultado_j['facc_'][sub,:]>=Acc_[sub]\n    Acc[sub] = np.mean(resultado_j['facc_'][sub,index_])\n    datos_ = Data_pow[sub,:,index_.reshape(-1),:,:]\n    temp = np.reshape(datos_,(datos_.shape[0]*datos_.shape[1]*datos_.shape[2]*\n                             datos_.shape[3]))\n    tam.append(temp.shape)\n    datos[sub,0:temp.shape[0]] = temp","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:22:56.562226Z","iopub.status.idle":"2021-09-02T13:22:56.562734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XT = datos\nYT = Acc.reshape(-1,1) # agrego el acc de los sujetos\nYT.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:22:56.563742Z","iopub.status.idle":"2021-09-02T13:22:56.564171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# normalizar ACC y filas de las características","metadata":{}},{"cell_type":"code","source":"#----------------------------------------------------------------------------------------------------------------\nind_YT = np.argsort(Acc).reshape(-1)\n# for each sample (row) we apply min-max transformation\nXT_norm_ = np.zeros((XT.shape))\nfor k in range(XT.shape[0]):\n  XT_norm_[k,:] = (XT[k,:]-XT[k,:].min())/(XT[k,:].max()-XT[k,:].min())\nXT_norm = XT_norm_[ind_YT,:]\n#----------------------------------------------------------------------------------------------------------------\n# for each sample (row) we apply min-max transformation\n# YTa = np.zeros((YT_.shape))\n# for k in range(YT_.shape[0]):\n#   YTa[k,:] = (YT_[k,:]-YT_[k,:].min())/(YT_[k,:].max()-YT_[k,:].min())\n# YT = YTa[ind_YT,:]\nYT = YT[ind_YT]\nYT.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:46:56.235589Z","iopub.execute_input":"2021-09-01T05:46:56.236063Z","iopub.status.idle":"2021-09-01T05:46:56.291002Z","shell.execute_reply.started":"2021-09-01T05:46:56.236013Z","shell.execute_reply":"2021-09-01T05:46:56.289881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# regresion SVR","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n# from sklearn.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.model_selection import LeaveOneOut, GridSearchCV, train_test_split, ShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn import svm, preprocessing\nfrom sklearn.base import  BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\n#this one to prevent warning about gamma settings\n# import warnings\n# warnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\n# data loading scaling, splitting\n# validation subject\nbest_ = list()\nresults_ = list()\n\nscorer = make_scorer(mean_squared_error)#, greater_is_better=False)\n# 30 trials donde se realiza una pertición de 3 grupos iguales, donde los dos primeros\n# se utilizaron para entrenamiento y el último de validación \n# clases de la base de datos.\ny = YT\n# PSD\nX = XT_norm\n# PAC in the time.\n# dat = np.squeeze()\n# X = np.reshape(dat,(dat.shape[0],dat.shape[1]*dat.shape[2]))\n# Grid_s = list()\n# for i in range(50):\n# print(f'fold '+str(i))\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state=0)\nshuffle_split = ShuffleSplit(n_splits=10, test_size=0.3,random_state=0)\n# shuffle_split = StratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n# cv = \nsteps = [('Reg', SVR(kernel='rbf'))]\npipe_svr = Pipeline(steps)\nparams_grid={\n          'Reg__C': [0.01, 0.1, 1, 100, 1000],\n          'Reg__gamma':[0.01, 0.1, 1, 100, 1000]\n          }\n\ngrid = GridSearchCV(estimator=pipe_svr, \n                  param_grid=params_grid, \n                  scoring=scorer,\n                  cv =shuffle_split,\n                  # LeaveOneOut(),\n                  error_score='raise',\n                  verbose=40,n_jobs=-1)\n\ngrid.fit(X,y.ravel())\nresults_.append(grid.best_estimator_.predict(X))#(grid.cv_results_)\nbest_.append(grid.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:46:56.293012Z","iopub.execute_input":"2021-09-01T05:46:56.293451Z","iopub.status.idle":"2021-09-01T05:47:01.671843Z","shell.execute_reply.started":"2021-09-01T05:46:56.293403Z","shell.execute_reply":"2021-09-01T05:47:01.670699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE = list()\nR2  = list()\nfor sub in range(len(best_)):\n    y_true,y_pred = YT,results_[sub]\n    mae = mean_absolute_error(y_true,y_pred)\n    r2  = r2_score(y_true,y_pred)\n    MAE.append(mae)\n    R2.append(r2)\n    print('Sujeto {sub}: Promedio MAE es {mae}, y R2 es {r2}'.format(sub=sub+1,mae=mae,r2=r2))\nprint('Sujetos: Promedio MAE es {mae}, y R2 es {r2}'.format(mae=np.mean(MAE),r2=np.mean(R2)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PAC","metadata":{}},{"cell_type":"code","source":"#filtering functions\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n  #lowcut is the lower bound of the frequency that we want to isolate\n  #hicut is the upper bound of the frequency that we want to isolate\n  #fs is the sampling rate of our data\n  nyq = 0.5 * fs #nyquist frequency - see http://www.dspguide.com/ if you want more info\n  low = float(lowcut) / nyq\n  high = float(highcut) / nyq\n  b, a = butter(order, [low, high], btype='band')\n  return b, a\n\ndef butter_bandpass_filter(mydata, lowcut, highcut, fs, order=4):\n  b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n  y = filtfilt(b, a, mydata)\n  return y","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:23:46.72531Z","iopub.execute_input":"2021-09-02T13:23:46.725676Z","iopub.status.idle":"2021-09-02T13:23:46.734514Z","shell.execute_reply.started":"2021-09-02T13:23:46.725642Z","shell.execute_reply":"2021-09-02T13:23:46.733121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import angle, sin, cos, sqrt\ndef moving_multitrial_pac(data1,data2,fr1,fr2,sampling_freq,type):\n  #calculating phase of theta\n  phase_data = butter_bandpass_filter(data1, fr1[0], fr1[1],(sampling_freq))\n  phase_data = angle(hilbert(phase_data))\n\n  #calculating amplitude envelope of high gamma\n  amp_data = butter_bandpass_filter(data2, fr2[0], fr2[1], (sampling_freq))\n  amp_data = abs(hilbert(amp_data))\n\n  # Filtering the amplitude of analytic signal of the high-frequency range within the\n  # frequency range of the low-frequency band\n  lowfromhigh = butter_bandpass_filter(amp_data, fr1[0], fr1[1],(sampling_freq))\n  \n  # lowfromhigh.shape\n  low_Env_high_filtered_signals = lowfromhigh-np.tile(np.mean(lowfromhigh),(Ntrials,1))\n  \n  # get the phase\n  Amp_phase_signals=angle(hilbert(low_Env_high_filtered_signals))\n  Ntime = 1*sampling_freq #data1.shape[1]\n  Nstep = 0.1*sampling_freq\n  windows = list(np.arange(0,data_ref[0].shape[2]-window,step))\n  Nsegments = len(windows)\n\n  if type == 'trials':\n    temporal_ = np.zeros((Amp_phase_signals.shape[0],Nsegments))\n  else:\n    temporal_ = np.zeros((Nsegments))\n  for ii in range(Nsegments): # repear over different segments\n    start = int(windows[ii])\n    stop  = int(windows[ii]+Ntime)\n    if type == 'trials':\n      plv = np.abs(np.sum(np.exp(1j*(phase_data[:,start:stop]-Amp_phase_signals[:,start:stop])),axis=1)/((Nstep)))\n      temporal_[:,ii]=plv\n    else:\n      plv = np.abs(np.sum(np.sum(np.exp(1j*(phase_data[:,start:stop]-Amp_phase_signals[:,start:stop]))))/(Ntrials*(Nstep)))\n      temporal_[ii]=plv\n  return temporal_","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:24:11.553116Z","iopub.execute_input":"2021-09-02T13:24:11.553506Z","iopub.status.idle":"2021-09-02T13:24:11.566053Z","shell.execute_reply.started":"2021-09-02T13:24:11.553475Z","shell.execute_reply":"2021-09-02T13:24:11.565281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# parámetros de PAC","metadata":{}},{"cell_type":"code","source":"# Parameters\n# Bands of rytms: delta to gamma.\nphase_providing_band = [[8,12]] \namplitude_providing_band = [[8,12]]\nNtrials   = data_ref[0].shape[0]\n# Nsegments = 10 # Number of segments in window.\nNchannels = data_ref[0].shape[1]\nNfreqs    = len(phase_providing_band)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:24:15.439764Z","iopub.execute_input":"2021-09-02T13:24:15.440281Z","iopub.status.idle":"2021-09-02T13:24:15.445031Z","shell.execute_reply.started":"2021-09-02T13:24:15.440249Z","shell.execute_reply":"2021-09-02T13:24:15.44422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Texto de título predeterminado\n# temporal_plv = np.zeros((len(Subjects),int(Nchannels*(Nchannels-1)/2),Nfreqs,Nfreqs,Nsegments))\nNsegments=85\ntemporal_plv = np.zeros((len(Subjects),Ntrials,int(Nchannels*(Nchannels-1)/2),Nfreqs,Nfreqs,Nsegments))\n# (Nodes,Nodes,Time)\n\nfor sub in range(len(Subjects)):\n  cont_ch = 0 # contador de las subredes\n  for ch1 in range(Nchannels):\n    for ch2 in range(ch1+1,Nchannels-1):\n      if ch1 != ch2:\n        data1= np.squeeze(data_ref[sub][:,ch1,:])\n        data2= np.squeeze(data_ref[sub][:,ch2,:])\n        fr_1 = 0\n        for fr1 in phase_providing_band:\n          fr_2 = 0\n          for fr2 in amplitude_providing_band:\n            temporal_plv[sub,:,cont_ch,fr_1,fr_2,:] = moving_multitrial_pac(data1,data2,fr1,fr2,sampling_freq,type='trials')\n            fr_2 += 1\n          fr_1 += 1\n      cont_ch += 1\n  # savemat(load_path+f'PLV_pac'+str(sub)+'_trials.mat',{'temporal_plv':temporal_plv})","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:24:43.493198Z","iopub.execute_input":"2021-09-02T13:24:43.493584Z","iopub.status.idle":"2021-09-02T13:38:35.336474Z","shell.execute_reply.started":"2021-09-02T13:24:43.493553Z","shell.execute_reply":"2021-09-02T13:38:35.335527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.squeeze(temporal_plv).shape","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:44:53.275858Z","iopub.execute_input":"2021-09-02T13:44:53.276231Z","iopub.status.idle":"2021-09-02T13:44:53.282972Z","shell.execute_reply.started":"2021-09-02T13:44:53.276197Z","shell.execute_reply":"2021-09-02T13:44:53.282073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"met = 1\n# Acc = np.zeros()\ndat = np.squeeze(temporal_plv)\nif met == 1:\n  Acc = Acc_\n  datos = np.reshape(dat,(dat.shape[0],dat.shape[1]*\n                               dat.shape[2]*dat.shape[3]))\nelif met == 2:\n  # datos = np.zeros((21,912000))\n  Acc = np.zeros((21,1))\n  index = list()\n  for sub in range(len(Acc_)):\n    index.append(resultado_j['facc_'][sub,:]>=Acc_[sub])\n  index_ = np.sum(np.asarray(index),axis=0).reshape(-1,1).T>10\n  Acc = np.mean(resultado_j['facc_'][:,index_.reshape(-1)],axis=1)\n  datos_ = dat[:,:,:,index_.reshape(-1)]\n  datos = np.reshape(datos_,(datos_.shape[0],datos_.shape[1]*datos_.shape[2]*\n                             datos_.shape[3]))\nelse:\n  datos = np.zeros((21,912000))\n  Acc = np.zeros((21,1))\n  index = list()\n  tam = list()\n  for sub in range(len(Acc_)):\n    index_ = resultado_j['facc_'][sub,:]>=Acc_[sub]\n    Acc[sub] = np.mean(resultado_j['facc_'][sub,index_])\n    datos_ = dat[sub,:,:,:,index_]\n    temp = np.reshape(datos_,(datos_.shape[0]*datos_.shape[1]*datos_.shape[2]*\n                             datos_.shape[3]))\n    tam.append(temp.shape)\n    datos[sub,0:temp.shape[0]] = temp","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:45:02.989351Z","iopub.execute_input":"2021-09-02T13:45:02.989728Z","iopub.status.idle":"2021-09-02T13:45:03.003252Z","shell.execute_reply.started":"2021-09-02T13:45:02.989692Z","shell.execute_reply":"2021-09-02T13:45:03.002033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XT = datos\nYT = Acc.reshape(-1,1) # agrego el acc de los sujetos\nYT.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:45:07.358764Z","iopub.execute_input":"2021-09-02T13:45:07.359325Z","iopub.status.idle":"2021-09-02T13:45:07.36682Z","shell.execute_reply.started":"2021-09-02T13:45:07.359275Z","shell.execute_reply":"2021-09-02T13:45:07.365542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#----------------------------------------------------------------------------------------------------------------\nind_YT = np.argsort(Acc).reshape(-1)\n# for each sample (row) we apply min-max transformation\nXT_norm_ = np.zeros((XT.shape))\nfor k in range(XT.shape[0]):\n  XT_norm_[k,:] = (XT[k,:]-XT[k,:].min())/(XT[k,:].max()-XT[k,:].min())\nXT_norm = XT_norm_[ind_YT,:]\n#----------------------------------------------------------------------------------------------------------------\n# for each sample (row) we apply min-max transformation\n# YTa = np.zeros((YT_.shape))\n# for k in range(YT_.shape[0]):\n#   YTa[k,:] = (YT_[k,:]-YT_[k,:].min())/(YT_[k,:].max()-YT_[k,:].min())\n# YT = YTa[ind_YT,:]\nYT = YT[ind_YT]\nYT.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:45:11.694022Z","iopub.execute_input":"2021-09-02T13:45:11.694532Z","iopub.status.idle":"2021-09-02T13:45:11.828461Z","shell.execute_reply.started":"2021-09-02T13:45:11.694498Z","shell.execute_reply":"2021-09-02T13:45:11.827748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n# from sklearn.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.model_selection import LeaveOneOut, GridSearchCV, train_test_split, ShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn import svm, preprocessing\nfrom sklearn.base import  BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\n#this one to prevent warning about gamma settings\n# import warnings\n# warnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\n# data loading scaling, splitting\n# validation subject\nbest_ = list()\nresults_ = list()\n\nscorer = make_scorer(mean_squared_error)#, greater_is_better=False)\n# 30 trials donde se realiza una pertición de 3 grupos iguales, donde los dos primeros\n# se utilizaron para entrenamiento y el último de validación \n# clases de la base de datos.\ny = YT\n# PSD\nX = XT_norm\n# PAC in the time.\n# dat = np.squeeze()\n# X = np.reshape(dat,(dat.shape[0],dat.shape[1]*dat.shape[2]))\n# Grid_s = list()\n# for i in range(50):\n# print(f'fold '+str(i))\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state=0)\nshuffle_split = ShuffleSplit(n_splits=10, test_size=0.3,random_state=0)\n# shuffle_split = StratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n# cv = \nsteps = [('Reg', SVR(kernel='rbf'))]\npipe_svr = Pipeline(steps)\nparams_grid={\n          'Reg__C': [0.01, 0.1, 1, 100, 1000],\n          'Reg__gamma':[0.01, 0.1, 1, 100, 1000]\n          }\n\ngrid = GridSearchCV(estimator=pipe_svr, \n                  param_grid=params_grid, \n                  scoring=scorer,\n                  cv =shuffle_split,\n                  # LeaveOneOut(),\n                  error_score='raise',\n                  verbose=20,n_jobs=-1)\n\ngrid.fit(X,y.ravel())\nresults_.append(grid.best_estimator_.predict(X))#(grid.cv_results_)\nbest_.append(grid.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:45:14.350894Z","iopub.execute_input":"2021-09-02T13:45:14.351433Z","iopub.status.idle":"2021-09-02T13:45:26.738048Z","shell.execute_reply.started":"2021-09-02T13:45:14.351368Z","shell.execute_reply":"2021-09-02T13:45:26.736813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE = list()\nR2  = list()\nfor sub in range(len(best_)):\n    y_true,y_pred = YT,results_[sub]\n    mae = mean_absolute_error(y_true,y_pred)\n    r2  = r2_score(y_true,y_pred)\n    MAE.append(mae)\n    R2.append(r2)\n    print('Sujeto {sub}: Promedio MAE es {mae}, y R2 es {r2}'.format(sub=sub+1,mae=mae,r2=r2))\nprint('Sujetos: Promedio MAE es {mae}, y R2 es {r2}'.format(mae=np.mean(MAE),r2=np.mean(R2)))","metadata":{"execution":{"iopub.status.busy":"2021-09-02T13:45:29.330577Z","iopub.execute_input":"2021-09-02T13:45:29.331089Z","iopub.status.idle":"2021-09-02T13:45:29.340358Z","shell.execute_reply.started":"2021-09-02T13:45:29.331043Z","shell.execute_reply":"2021-09-02T13:45:29.339614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sujeto 1: Promedio MAE es 0.05418981481481479, y R2 es 0.24613730264050303\nSujetos: Promedio MAE es 0.05418981481481479, y R2 es 0.24613730264050303","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sujeto 1: Promedio MAE es 0.05418981481481479, y R2 es 0.24613730264050303","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}